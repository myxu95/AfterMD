# AfterMD é›†ç¾¤æ‰¹é‡å¤„ç†æŒ‡å—

## æ¦‚è¿°

AfterMD æä¾›äº†å¼ºå¤§çš„ SLURM è„šæœ¬ç”ŸæˆåŠŸèƒ½ï¼Œå¯ä»¥å°†å¤§é‡çš„ MD æ¨¡æ‹Ÿä»»åŠ¡è‡ªåŠ¨åˆ†ç»„å¹¶ç”Ÿæˆç›¸åº”çš„é›†ç¾¤ä½œä¸šè„šæœ¬ã€‚è¿™ä½¿å¾—åœ¨ HPC é›†ç¾¤ä¸Šæ‰¹é‡å¤„ç†æ•°ç™¾ä¸ª MD è½¨è¿¹å˜å¾—ç®€å•é«˜æ•ˆã€‚

## æ ¸å¿ƒåŠŸèƒ½

### ğŸ”„ ä»»åŠ¡åˆ†ç»„
- è‡ªåŠ¨å°†å¤§é‡ MD ä»»åŠ¡åˆ†ç»„æˆåˆé€‚çš„æ‰¹æ¬¡
- ä¾‹å¦‚ï¼š20 ä¸ªä»»åŠ¡ â†’ æ¯æ‰¹ 10 ä¸ª â†’ ç”Ÿæˆ 2 ä¸ª SLURM è„šæœ¬
- æ”¯æŒä¸å‡åŒ€åˆ†ç»„ï¼ˆå¦‚ 23 ä¸ªä»»åŠ¡ â†’ 2Ã—10 + 1Ã—3ï¼‰

### âš™ï¸ SLURM å‚æ•°å®šåˆ¶
- çµæ´»é…ç½®åˆ†åŒºã€æ—¶é—´ã€CPUã€GPU ç­‰èµ„æº
- æ”¯æŒ conda ç¯å¢ƒæ¿€æ´»
- è‡ªåŠ¨å¤„ç† GROMACS æ¨¡å—åŠ è½½
- åŸºäºæ¨¡æ¿çš„è„šæœ¬ç”Ÿæˆ

### ğŸš€ è‡ªåŠ¨åŒ–æäº¤
- ç”Ÿæˆæ‰¹é‡æäº¤è„šæœ¬
- å¯é…ç½®ä½œä¸šé—´æäº¤å»¶è¿Ÿ
- é›†æˆä½œä¸šçŠ¶æ€ç›‘æ§å‘½ä»¤

## åŸºæœ¬ä½¿ç”¨æ–¹æ³•

### 1. æœ€ç®€å•çš„ä½¿ç”¨

```python
from aftermd.utils.slurm_generator import generate_slurm_scripts_for_md_tasks

# ä¸º 20 ä¸ª MD ä»»åŠ¡ç”Ÿæˆ SLURM è„šæœ¬ï¼Œæ¯æ‰¹å¤„ç† 10 ä¸ª
results = generate_slurm_scripts_for_md_tasks(
    simulations_path="/data/md_simulations",
    tasks_per_batch=10
)

print(f"ç”Ÿæˆäº† {len(results['scripts'])} ä¸ª SLURM è„šæœ¬")
```

### 2. è‡ªå®šä¹‰é›†ç¾¤å‚æ•°

```python
# å®šä¹‰é›†ç¾¤å‚æ•°
slurm_params = {
    "partition": "gpu",
    "time": "24:00:00", 
    "cpus_per_task": 16,
    "gres": "gpu:2",
    "memory": "64G",
    "conda_env": "aftermd"
}

results = generate_slurm_scripts_for_md_tasks(
    simulations_path="/data/md_simulations",
    tasks_per_batch=5,
    slurm_params=slurm_params,
    dt=10.0  # è½¨è¿¹é™é‡‡æ ·ï¼Œæ¯ 10 ps ä¸€å¸§
)
```

### 3. å‘½ä»¤è¡Œä½¿ç”¨

```bash
# åŸºæœ¬ç”¨æ³•
python -m aftermd.utils.slurm_generator /data/simulations --tasks-per-batch 10

# è‡ªå®šä¹‰å‚æ•°
python -m aftermd.utils.slurm_generator /data/simulations \
  --tasks-per-batch 5 \
  --partition gpu \
  --time 24:00:00 \
  --cpus-per-task 16 \
  --conda-env aftermd

# ä½¿ç”¨è‡ªå®šä¹‰æ¨¡æ¿
python -m aftermd.utils.slurm_generator /data/simulations \
  --template my_slurm_template.sh
```

## ä»»åŠ¡åˆ†ç»„ç­–ç•¥

### åˆ†ç»„è®¡ç®—ç¤ºä¾‹

| æ€»ä»»åŠ¡æ•° | æ¯æ‰¹ä»»åŠ¡æ•° | ç”Ÿæˆè„šæœ¬æ•° | åˆ†ç»„ç»“æœ |
|---------|----------|----------|----------|
| 20      | 10       | 2        | 10 + 10 |
| 23      | 10       | 3        | 10 + 10 + 3 |
| 100     | 15       | 7        | 6Ã—15 + 1Ã—10 |
| 5       | 10       | 1        | 5 |

### é€‰æ‹©åˆé€‚çš„æ‰¹æ¬¡å¤§å°

**è€ƒè™‘å› ç´ ï¼š**
- **é›†ç¾¤èµ„æº**: CPU/GPU æ ¸å¿ƒæ•°ï¼Œå†…å­˜å¤§å°
- **ä»»åŠ¡å¤æ‚åº¦**: è½¨è¿¹å¤§å°ï¼Œå¤„ç†æ—¶é—´
- **é˜Ÿåˆ—é™åˆ¶**: æœ€å¤§ä½œä¸šæ—¶é—´ï¼Œå¹¶å‘ä½œä¸šæ•°
- **å­˜å‚¨ I/O**: é¿å…è¿‡å¤šå¹¶å‘è¯»å†™

**æ¨èç­–ç•¥ï¼š**
```python
# å°ä»»åŠ¡ï¼ˆ< 1GB è½¨è¿¹ï¼‰
tasks_per_batch = 20

# ä¸­ç­‰ä»»åŠ¡ï¼ˆ1-5GB è½¨è¿¹ï¼‰
tasks_per_batch = 10

# å¤§ä»»åŠ¡ï¼ˆ> 5GB è½¨è¿¹ï¼‰
tasks_per_batch = 5

# è¶…å¤§ä»»åŠ¡ï¼ˆ> 20GB è½¨è¿¹ï¼‰
tasks_per_batch = 1
```

## ç”Ÿæˆçš„è„šæœ¬ç»“æ„

### å…¸å‹ SLURM è„šæœ¬å†…å®¹

```bash
#!/bin/bash
#SBATCH --job-name=aftermd_batch_001
#SBATCH -p gpu
#SBATCH --time=24:00:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --gres=gpu:2
#SBATCH --mem=64G

# Activate conda environment
source $(conda info --base)/etc/profile.d/conda.sh
conda activate aftermd
echo "Activated conda environment: aftermd"

# Environment setup
export LD_LIBRARY_PATH=/public/software/lib/:$LD_LIBRARY_PATH
source /public/software/profile.d/apps_gromacs_2023.2.sh

# Job information
echo "Start time: $(date)"
echo "SLURM_JOB_NODELIST: $SLURM_JOB_NODELIST" 
echo "hostname: $(hostname)"
echo "CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"
echo "Job directory: $(pwd)"
echo "Processing 5 MD tasks in this batch"
echo ""

# AfterMD batch processing
python -m aftermd.batch_process "/data/md_simulations" \
  --output "/data/processed" \
  --dt 10.0 \
  --verbose

echo ""
echo "End time: $(date)"
echo "Batch processing completed"
```

### æäº¤è„šæœ¬ (submit_all_batches.sh)

```bash
#!/bin/bash
# AfterMD Batch Job Submission Script

echo "Submitting 4 AfterMD batch jobs..."

# Submit batch 1/4
echo "Submitting aftermd_batch_001.sh..."
sbatch "aftermd_batch_001.sh"
sleep 5

# Submit batch 2/4
echo "Submitting aftermd_batch_002.sh..."
sbatch "aftermd_batch_002.sh"
sleep 5

# ... (ç»§ç»­å…¶ä»–æ‰¹æ¬¡)

echo "All 4 batch jobs submitted!"
echo "Use 'squeue -u $USER' to check job status"
```

## å®é™…ä½¿ç”¨å·¥ä½œæµ

### æ­¥éª¤ 1: å‡†å¤‡æ•°æ®

```bash
# ç¡®ä¿ MD æ•°æ®ç»„ç»‡æ­£ç¡®
ls /data/md_simulations/
# task_001/md.xtc, md.tpr
# task_002/prod/md.xtc, md.tpr
# task_003/md.xtc, md.tpr
# ...
```

### æ­¥éª¤ 2: ç”Ÿæˆ SLURM è„šæœ¬

```python
from aftermd.utils.slurm_generator import generate_slurm_scripts_for_md_tasks

results = generate_slurm_scripts_for_md_tasks(
    simulations_path="/data/md_simulations",
    tasks_per_batch=10,
    output_script_dir="/home/user/slurm_scripts",
    slurm_params={
        "partition": "gpu",
        "time": "12:00:00",
        "conda_env": "aftermd"
    }
)
```

### æ­¥éª¤ 3: æ£€æŸ¥ç”Ÿæˆçš„è„šæœ¬

```bash
cd /home/user/slurm_scripts
ls -la
# aftermd_batch_001.sh
# aftermd_batch_002.sh  
# submit_all_batches.sh

# æ£€æŸ¥è„šæœ¬å†…å®¹
head -20 aftermd_batch_001.sh
```

### æ­¥éª¤ 4: æäº¤ä½œä¸š

```bash
# æäº¤æ‰€æœ‰æ‰¹æ¬¡
bash submit_all_batches.sh

# æˆ–è€…é€ä¸ªæäº¤
sbatch aftermd_batch_001.sh
sbatch aftermd_batch_002.sh
```

### æ­¥éª¤ 5: ç›‘æ§ä½œä¸š

```bash
# æŸ¥çœ‹ä½œä¸šçŠ¶æ€
squeue -u $USER

# æŸ¥çœ‹ä½œä¸šè¯¦æƒ…
scontrol show job <job_id>

# æŸ¥çœ‹è¾“å‡ºæ—¥å¿—
tail -f slurm-<job_id>.out
```

### æ­¥éª¤ 6: æ”¶é›†ç»“æœ

```bash
# æ£€æŸ¥å¤„ç†ç»“æœ
ls /data/processed/
# task_001/task_001_processed.xtc
# task_002/task_002_processed.xtc
# ...

# æ£€æŸ¥å¤„ç†æ—¥å¿—
grep -r "successfully" /data/processed/*/processing_metadata.json
```

## é«˜çº§é…ç½®

### è‡ªå®šä¹‰ SLURM æ¨¡æ¿

åˆ›å»ºè‡ªå®šä¹‰æ¨¡æ¿æ–‡ä»¶ `my_template.sh`:

```bash
#!/bin/bash
#SBATCH --job-name={job_name}
#SBATCH --partition={partition}
#SBATCH --time={time}
#SBATCH --nodes={nodes}
#SBATCH --ntasks={ntasks}
#SBATCH --cpus-per-task={cpus_per_task}
{gres_line}
{memory_line}
#SBATCH --output=aftermd_%j.out
#SBATCH --error=aftermd_%j.err

{conda_activation}

# è‡ªå®šä¹‰ç¯å¢ƒè®¾ç½®
module load gromacs/2023.2
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

# å¤„ç†å‘½ä»¤
{processing_commands}
```

ä½¿ç”¨è‡ªå®šä¹‰æ¨¡æ¿ï¼š

```python
results = generate_slurm_scripts_for_md_tasks(
    simulations_path="/data/simulations",
    template_file="my_template.sh",
    tasks_per_batch=10
)
```

### ä¸åŒé›†ç¾¤é…ç½®ç¤ºä¾‹

#### GPU é›†ç¾¤é…ç½®
```python
gpu_params = {
    "partition": "gpu",
    "time": "24:00:00",
    "cpus_per_task": 8,
    "gres": "gpu:1",
    "memory": "32G"
}
```

#### CPU å¯†é›†å‹é…ç½®
```python
cpu_params = {
    "partition": "cpu",
    "time": "48:00:00", 
    "cpus_per_task": 32,
    "memory": "128G"
}
```

#### å¿«é€Ÿé˜Ÿåˆ—é…ç½®
```python
fast_params = {
    "partition": "fast",
    "time": "4:00:00",
    "cpus_per_task": 16
}
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **SLURM å‚æ•°ä¸å…¼å®¹**
   ```bash
   # æ£€æŸ¥é›†ç¾¤åˆ†åŒºä¿¡æ¯
   sinfo
   
   # æ£€æŸ¥èµ„æºé™åˆ¶
   scontrol show partition <partition_name>
   ```

2. **GROMACS æ¨¡å—åŠ è½½å¤±è´¥**
   ```bash
   # æ£€æŸ¥å¯ç”¨æ¨¡å—
   module avail gromacs
   
   # æµ‹è¯•æ¨¡å—åŠ è½½
   module load gromacs/2023.2
   gmx --version
   ```

3. **Conda ç¯å¢ƒé—®é¢˜**
   ```bash
   # éªŒè¯ç¯å¢ƒå­˜åœ¨
   conda env list
   
   # æµ‹è¯•ç¯å¢ƒæ¿€æ´»
   conda activate aftermd
   python -c "import aftermd; print('OK')"
   ```

4. **æ–‡ä»¶è·¯å¾„è®¿é—®é—®é¢˜**
   ```bash
   # æ£€æŸ¥è·¯å¾„æƒé™
   ls -la /data/md_simulations/
   
   # æµ‹è¯•æ–‡ä»¶å‘ç°
   python -c "from aftermd import discover_md_tasks; print(discover_md_tasks('/data/simulations'))"
   ```

### è°ƒè¯•æŠ€å·§

1. **å…ˆæµ‹è¯•å°æ‰¹æ¬¡**
   ```python
   # å…ˆç”Ÿæˆ 1 ä¸ªä»»åŠ¡çš„è„šæœ¬è¿›è¡Œæµ‹è¯•
   results = generate_slurm_scripts_for_md_tasks(
       simulations_path="/data/simulations",
       tasks_per_batch=1
   )
   ```

2. **å¯ç”¨è¯¦ç»†è¾“å‡º**
   ```bash
   python -m aftermd.utils.slurm_generator /data/simulations --verbose
   ```

3. **æ£€æŸ¥ç”Ÿæˆçš„è„šæœ¬**
   ```bash
   # æ‰‹åŠ¨è¿è¡Œç”Ÿæˆçš„å‘½ä»¤è¿›è¡Œæµ‹è¯•
   bash -x aftermd_batch_001.sh
   ```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### èµ„æºé…ç½®ä¼˜åŒ–

1. **CPU æ ¸å¿ƒæ•°**: æ ¹æ® GROMACS æ€§èƒ½æµ‹è¯•ç¡®å®šæœ€ä¼˜æ ¸å¿ƒæ•°
2. **å†…å­˜åˆ†é…**: é€šå¸¸ 2-4GB æ¯ä¸ª CPU æ ¸å¿ƒ
3. **GPU é…ç½®**: å¯¹äºå¤§ç³»ç»Ÿä½¿ç”¨ GPU åŠ é€Ÿ
4. **I/O ä¼˜åŒ–**: é¿å…è¿‡å¤šå¹¶å‘è¯»å†™åŒä¸€å­˜å‚¨

### æ‰¹æ¬¡å¤§å°ä¼˜åŒ–

```python
# æ ¹æ®ç³»ç»Ÿå¤§å°è°ƒæ•´æ‰¹æ¬¡å¤§å°
def calculate_optimal_batch_size(total_tasks, avg_traj_size_gb):
    if avg_traj_size_gb < 1:
        return min(20, total_tasks)
    elif avg_traj_size_gb < 5:
        return min(10, total_tasks)
    else:
        return min(5, total_tasks)

optimal_batch = calculate_optimal_batch_size(100, 2.5)  # 100 ä¸ªä»»åŠ¡ï¼Œå¹³å‡ 2.5GB
```

## æ€»ç»“

AfterMD çš„ SLURM è„šæœ¬ç”Ÿæˆå™¨æä¾›äº†ï¼š

âœ… **è‡ªåŠ¨åŒ–ä»»åŠ¡åˆ†ç»„**: æ™ºèƒ½åˆ†é…å¤§é‡ MD ä»»åŠ¡åˆ°åˆé€‚çš„æ‰¹æ¬¡  
âœ… **çµæ´»çš„é›†ç¾¤é…ç½®**: æ”¯æŒå„ç§ HPC é›†ç¾¤ç¯å¢ƒ  
âœ… **å®Œæ•´çš„å·¥ä½œæµé›†æˆ**: ä»æ•°æ®å‘ç°åˆ°ç»“æœå¤„ç†çš„ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–  
âœ… **å¼ºå¤§çš„å®šåˆ¶èƒ½åŠ›**: æ¨¡æ¿ç³»ç»Ÿæ”¯æŒå„ç§ç‰¹æ®Šéœ€æ±‚  
âœ… **ç®€å•çš„ä½¿ç”¨æ¥å£**: å‡ è¡Œä»£ç å³å¯ç”Ÿæˆå®Œæ•´çš„ä½œä¸šè„šæœ¬  

è¿™ä½¿å¾—åœ¨é›†ç¾¤ä¸Šæ‰¹é‡å¤„ç†æ•°ç™¾ä¸ª MD è½¨è¿¹å˜å¾—ç®€å•ã€é«˜æ•ˆä¸”å¯é ã€‚